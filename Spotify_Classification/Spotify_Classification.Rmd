---
title: "Spotify Predictive Analysis"
author: "Alejandro Cepeda"
date: "6/16/2022"
output: 
  github_document:
    dev: jpeg
---

***GRAMMAR CHECK ALL MARKDOWN TEXT BEFORE UPLOADING***

## Context

![](spotify-logo.png)

Music (listening and playing) is one of my favorite pastimes and go-to therapy session to de-stress. From a very young age, I have always enjoyed diverse genres of music, which then prompted me to search for and discover new genres I may enjoy. That of course could be a hit or miss depending on what I discover. To combat the struggle of manually conducting searches for new or previously unknown genres of music, I ask the following question: **What determines a track's genre category?**. Knowing this may help me determine what makes a track enjoyable or not based on my own tastes and hopefully others who read this! To answer this, I downloaded a Spotify dataset from [Kaggle.com](https://www.kaggle.com/datasets/zaheenhamidani/ultimate-spotify-tracks-db), which was gathered using the Spotify API, mined, and saved by Kaggle user Zaheen Hamidani. This dataset contains track information and the audio features associated with their respective track, sourced from Spotify's API [documentation](https://developer.spotify.com/documentation/web-api/reference/#/operations/get-audio-features):

## Data Description {#description}

-   **genre**: Song genre
-   **artist_name**: Song artist
-   **track_name**: Song name
-   **track_id**: Song unique ID
-   **popularity**: Song popularity (0-100) where higher is better
-   **acousticness**: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.
-   **danceability**: Describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.
-   **duration_ms**: Duration of track in milliseconds
-   **energy**: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.
-   **instrumentalness**: Predicts whether a track contains no vocals. "Ooh" and "aah" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly "vocal". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.
-   **key**: The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C#/D#, 2 = D, and so on. If no key was detected, the value is -1.
-   **liveness**: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.
-   **loudness**: The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.
-   **mode**: Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.
-   **speechiness**: Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.
-   **tempo**: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.
-   **time_signature**: A measurement used in music to indicate meter, written as a fraction with the bottom number indicating the kind of note used as a unit of time and the top number indicating the number of units in each measure.
-   **valence**: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).

## Packages and Data Collection

The following are the packages used throughout this project:

```{r "Packages", message=FALSE}
# libraries
library(tidyverse)
library(corrplot)
library(rsample)
library(broom)
library(yardstick)
library(nnet)
library(Metrics)
library(ranger)
library(tidymodels)
```

```{r setup, include=FALSE}
# setting global theme across all viz
theme_set(theme_minimal())
```

Let's read in the dataset from the working directory:

```{r "Data Collection"}
# read in data
spotify <- read_csv("SpotifyFeatures.csv", show_col_types = FALSE)
glimpse(spotify)
```

## Data Cleaning

Prior to conducting EDA, there is already some preexisting knowledge regarding the variables within the dataframe. Since this data was gathered using an API, it is always a good practice to check for duplicate observations (or rows) to ensure uniformity. The variables `artist_name`, `track_name`, and `track_id` will not be needed for the purpose of this project, thus the best course of action may be to remove them. To conduct all of these data cleaning steps, I created a pipeline using the dplyr's `%>%` operator (loaded from the tidyverse package).

```{r "Data Preprocessing"}
# pipeline to clean, drop and reformat variables
spotify_clean <- spotify %>%
  # remove duplicate tracks
  distinct(track_id, .keep_all=TRUE) %>%
  # select relevant variables
  select(-c(artist_name, track_name, track_id)) %>%
  # convert character variables to factor
  mutate(genre = as_factor(genre),
         time_signature = as_factor(time_signature),
         # convert duration to minutes instead
         duration_min = duration_ms/60000) %>%
  select(-duration_ms)
```

Based on prior musical knowledge (guitar and percussion classes), I decided it is best to exclude `key`, `mode`, and `time_signature` from the dataframe as they do not properly distinguish a genre as their measurements often overlap, regardless of how a track sounds. The key, as described in the [data description](#description), measures the pitch the track was recorded in, which often only changes how high or low the pitch will sound. The mode, labelled as either major or minor, may not be specific enough for the classification models to properly differentiate one from the other to predict a music genre. Time signature will be used for further analysis as the tempo in which a track is played may be useful for classifying a genre.

```{r "Data Preprocessing 2"}
# drop mode and time signature
spotify_clean <- spotify_clean %>%
  select(-c(key, mode))
```

Before proceeding any further, inspecting the genres within the dataframe may be helpful in the case of a misleading genre being present.

```{r "Data Preprocessing 3"}
# ensure genres are properly written
levels(spotify_clean$genre)

# recode Children's Music genres as one
levels(spotify_clean$genre)[levels(spotify_clean$genre) == "Childrenâ€™s Music"] <- "Children's Music"

# narrowing down genres for less memory usage
spotify_clean <- spotify_clean %>%
  filter(genre %in% c("Country","Electronic","Hip-Hop",
                      "Classical","Reggae","Reggaeton","Jazz"))

# verify recoding of levels
levels(spotify_clean$genre)
```

Success! We can now continue with our cleaning process.

As the final step of the cleaning process, let's check for any missing values:

```{r "Data Preprocessing 4"}
# check for missing data
colSums(is.na(spotify_clean))
```

Great news, 0 NAs were found!

## Exploratory Data Analysis

Now that our data is clean, we can conduct some EDA and determine whether any further manipulation to be made on the data as well as selecting the most appropriate features for the models.

```{r "Exploratory Data Analysis"}
# number of tracks per genre
spotify_clean %>%
  group_by(genre) %>%
  count() %>%
  arrange(n)

# reorder variables
spotify_clean <- spotify_clean %>%
  # remove A Capella from data
  filter(!genre %in% c("A Capella","Anime","Children's Music","Movie","Soundtrack","World")) %>%
  droplevels() %>%
  select(genre, time_signature, everything())
```

### Feature Engineering

To ensure the data possesses proper center and spread, let's take a look at each possible features by genre. First let's visualize the distribution of each possible feature across all genres in the dataframe:

```{r "Feature Engineering", warning=FALSE, message=FALSE}
# store numeric variable names in a vector
feature_names <- names(spotify_clean)[3:13]

# density plot of numeric features by genre
spotify_clean %>%
  select(c(genre, feature_names)) %>%
  # convert data to long format based on features
  pivot_longer(cols = feature_names) %>%
  ggplot(aes(x = value, fill = genre)) +
  geom_density() +
  facet_wrap(~name, ncol = 3, scales = "free") +
  labs(title = "Spotify Audio Feature Density Across Genres",
       x = "", y = "density") +
  theme(axis.text.x = element_text(size = 8, angle = 40),
        axis.text.y = element_blank(),
        legend.title = element_text(size=10),
        legend.key.size = unit(5, "mm"),
        legend.text = element_text(size=8))
```

Based on the density plots above, `duration_min`, `instrumentalness`, and `loudness` require normalization to ensure a well distributed dataframe.

**Track Duration**

```{r "Track Duration"}
# look for outliers
spotify_clean %>%
  ggplot(aes(y = duration_min)) +
  geom_boxplot() + 
  coord_flip() +
  ggtitle("Duration (in minutes)")

# store outliers based on 4th whisker
duration_outliers <- boxplot(spotify_clean$duration_min, 
                             plot = FALSE, range = 4)$out

# remove outliers
spotify_clean <- spotify_clean %>%
  filter(!duration_min %in% duration_outliers)

spotify_clean %>%
  ggplot(aes(y = duration_min)) +
  geom_boxplot() + 
  coord_flip() +
  ggtitle("Duration(in minutes) - no outliers")
```

**Instrumentalness**

```{r "Instrumentalness", warning=FALSE}
# look for outliers
spotify_clean %>%
  ggplot(aes(y = instrumentalness)) +
  geom_boxplot() +
  coord_flip() +
  ggtitle("Instrumentalness")

# compare data w/ and w/o instrumentalness > 0
spotify_clean %>%
  filter(instrumentalness > 0.1) %>%
  ggplot(aes(y = instrumentalness)) +
  geom_boxplot() +
  coord_flip()

# add all tracks w/o instrumentalness by genre
spotify_clean %>% 
  group_by(genre) %>%
  summarize(sum(instrumentalness == 0))
```

From the yielded results above, I will remove `instrumentalness` as a predictor variable due to the little influence it shows in terms of classifying a genre.

```{r "Instrumentalness 2"}
spotify_clean <- spotify_clean %>%
  select(-instrumentalness)
```

**Loudness**

```{r "Loudness"}
# look for outliers
spotify_clean %>%
  ggplot(aes(y = loudness)) +
  geom_boxplot() +
  coord_flip() +
  ggtitle("Loudness")


# remove outliers
spotify_clean <- spotify_clean %>%
  filter(loudness < max(loudness))

# boxplot without outlier
spotify_clean %>%
  ggplot(aes(y = loudness)) +
  geom_boxplot() +
  coord_flip() +
  ggtitle("Loudness - no outliers")
```

**Time Signature**

```{r "Time Signature"}
# plot time_signature distribution
spotify_clean %>%
  ggplot(aes(x=time_signature)) +
  geom_bar()
```

As can be seen in the plot above, a substantial number of tracks were recorded to be in `4/4` meter, which can cause great bias during the model training process. Based on inference and industry knowledge, I will remove `time_signature` from the dataframe. 

```{r "Time Signature 2"}
# drop time_signature
spotify_clean <- spotify_clean %>%
  select(-time_signature)
```

### Correlation {#correlation}

The final step prior to the modeling process is to find any features to exclude with the help of a visualization of the correlations across all numerical features. What we are looking for here are associations between the features so any bias between multiple variables are avoided when training and testing our model.

```{r "Correlation"}
# correlation plot of numeric features
spotify_clean %>%
  select(-c(genre)) %>% 
  cor() %>%
  corrplot(method="number", type="upper", diag=FALSE,
           tl.col = "black", tl.cex=0.9, tl.srt=45)
```

Loudness and energy possess the highest positive correlation (0.86), therefore one must go in order to avoid prediction bias. Since `energy` is much more evenly distributed compared to `loudness`, the latter will be dropped from the final version of the full data frame.

```{r "Correlation 2"}
# remove loudness
spotify_final <- spotify_clean %>%
  select(-loudness)
```

## Data Pre-processing

### Train/Test/CV Split

```{r "Train/Test Split"}
# creating create initial split 
spotify_split <- initial_split(spotify_final, prop=0.75)

# create train and test sets
spotify_train <- training(spotify_split)
spotify_test <- testing(spotify_split)

# create 5 fold cv split
cv_split <- vfold_cv(spotify_train, v=10)

# store cv dataset
cv_data <- cv_split %>%
  mutate(train = map(splits, ~training(.x)),
         validate = map(splits, ~testing(.x)))
```

## Modeling



### Logistic Regression

```{r "Logistic Regression"}
# build multinom logistic regression model
cv_models_mlr <- cv_data %>%
  mutate(model = map(train, ~multinom(genre~., data=.x)))
```



**Model Performance**

```{r "MLR Model Performance"}
# Prepare actual test set classes
mlr_test_actual <- spotify_test$genre

# Prepare vector of predicted values
mlr_test_predicted <- predict(cv_models_mlr$model[[2]], spotify_test, type = "class")
```

**Test Set Performance**

```{r "MLR Test Set Performance"}
# Compare the actual & predicted performance visually using a table
table(mlr_test_actual, mlr_test_predicted)

# Calculate the accuracy
accuracy(mlr_test_actual, mlr_test_predicted)
```


### Random Forest

```{r "Random Forest"}
# determine tuning params before modeling
cv_tune_rf <- cv_data %>%
  crossing(mtry = c(1:5))

# build rf model
cv_models_rf <- cv_tune_rf %>% 
  mutate(model = map2(train, mtry, ~ranger(formula = genre~., 
                                           data = .x, mtry = .y,
                                           num.trees = 100, seed = 84)))

```

**Model Tuning**

```{r "RF Model Tuning"}
# Generate validate predictions for each model
cv_prep_rf <- cv_models_rf %>% 
  mutate(validate_actual = map(validate, ~.x$genre),
         validate_predicted = map2(.x = model, .y = validate, 
                                   ~predict(.x, .y, type = "response")$predictions))

# Calculate the validate recall for each cross validation fold
cv_perf_acc_rf <- cv_prep_rf %>% 
  mutate(validate_acc = map2_dbl(validate_actual, validate_predicted, 
                                    ~accuracy(actual = .x, predicted = .y)))

# Calculate the mean recall for each mtry used  
cv_perf_acc_rf %>% 
  group_by(mtry) %>% 
  summarize(mean_acc = mean(validate_acc))
```

**Model Performance**

```{r "RF Model Performance"}
# Build the logistic regression model using all training data
rf_best_model <- ranger(genre~., data = spotify_train, 
                     num.trees = 500, mtry = 2)

# Prepare binary vector of actual Attrition values for testing_data
rf_test_actual <- spotify_test$genre

# Prepare binary vector of predicted Attrition values for testing_data
rf_test_predicted <- predict(rf_best_model, spotify_test, type = "response")$predictions
```

**Test Set Performance**

```{r "RF Test Set Performance"}
# Compare the actual & predicted performance visually using a table
table(rf_test_actual, rf_test_predicted)

# Calculate the test accuracy
accuracy(rf_test_actual, rf_test_predicted)
```


### Model Comparison

```{r "MLR Confusion Matrix"}
# store multinom confusion matrix
mlr_confmat <- as.data.frame(table(mlr_test_actual, mlr_test_predicted))

# visualize confusion matrix
mlr_confmat %>%
  ggplot(aes(mlr_test_actual, mlr_test_predicted)) +
  geom_tile(aes(fill = Freq)) +
  geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
  scale_fill_gradient(low = "white",
                      high = "red") +
  labs(x = "Actual", y = "Predicted")
```



```{r "RF Confusion Matrix"}
# store random forest confusion matrix
rf_confmat <- as.data.frame(table(rf_test_actual, rf_test_predicted))

# visualize confusion matrix
rf_confmat %>%
  ggplot(aes(rf_test_actual, rf_test_predicted)) +
  geom_tile(aes(fill = Freq)) +
  geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
  scale_fill_gradient(low = "white",
                      high = "red") +
  labs(x = "Actual", y = "Predicted")
```

## Conclusion

